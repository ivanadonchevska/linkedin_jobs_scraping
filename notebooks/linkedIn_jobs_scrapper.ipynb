{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Scrape LinkedIn using Selenium, Request and BeautifulSoup \n",
    "\n",
    "The folowing details will be scraped:\n",
    "- job id\n",
    "- job title\n",
    "- seniority level\n",
    "- location\n",
    "- job description\n",
    "- number of candidates\n",
    "- posted time ago"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To scrape the job ids, will use Selenium to navigate to: https://www.linkedin.com/jobs/search?\n",
    "- LinkedIn credentials and having chromedriver executable are required\n",
    "- And after that scraping other job details (level, description...) will be done using simple GET request, from the requests library, using it's already collected job id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Scraping LinkedIn Jobs Ids using Selenium and BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import requests\n",
    "\n",
    "import time, datetime\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import math, re, sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    LogIn to LinkedIn using selenium\n",
    "\n",
    "LinkedIn credentials will be saved in '../data/user_credentials.txt', so can access and change them from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('email', 'password')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/user_credentials.txt', 'r', encoding=\"utf-8\") as file:\n",
    "    user_credentials = file.readlines()\n",
    "    user_credentials = [line.rstrip() for line in user_credentials]\n",
    "\n",
    "email,passwd = user_credentials[0],user_credentials[1]\n",
    "email, passwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Set up the chromedriver and open the linkedIn page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_driver_path = '../chromedriver-mac-arm64/chromedriver'\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "driver = webdriver.Chrome(options=options, service=service)\n",
    "\n",
    "driver.get('https://www.linkedin.com/login')\n",
    "time.sleep(5) # waiting page to load\n",
    "\n",
    "email_input = driver.find_element(By.ID, 'username')\n",
    "password_input = driver.find_element(By.ID, 'password')\n",
    "email_input.send_keys(email)\n",
    "password_input.send_keys(passwd)\n",
    "\n",
    "# Click the login button\n",
    "password_input.send_keys(Keys.ENTER)\n",
    "\n",
    "time.sleep(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Scrapping Linkedin Jobs IDs\n",
    "\n",
    "Should provide search parameters, like job title and location. In LinkedIn search results are displayed on many pages, where 25 jobs are listed on each page. So to navigate on each page will use start parameter and scroll to the bottom on each page, so the full data can be loaded. And at the end to get the searched job ids will parse the HTML content using BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_to_bottom(driver,sleep_time=120):\n",
    "    last_height = driver.execute_script('return document.body.scrollHeight')\n",
    "    while True:\n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        new_height = driver.execute_script('return document.body.scrollHeight')\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "    time.sleep(sleep_time)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = 'junior%20developer'\n",
    "location = 'Sofia'\n",
    "start = 0\n",
    "\n",
    "url = f'https://linkedin.com/jobs/search/?keywords={keywords}&location={location}&start={start}'\n",
    "url = requests.utils.requote_uri(url)\n",
    "driver.get(url)\n",
    "scroll_to_bottom(driver, sleep_time=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the HTML content of the page and get number of jobs and pages as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_jobs: 38\n",
      "number_of_pages: 2\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "try:\n",
    "    div_number_of_jobs = soup.find(\"div\",{\"class\":\"jobs-search-results-list__subtitle\"})\n",
    "    number_of_jobs = int(div_number_of_jobs.find('span').get_text().strip().split()[0])\n",
    "except:\n",
    "    number_of_jobs = 0\n",
    "    \n",
    "number_of_pages=math.ceil(number_of_jobs/25)\n",
    "print(\"number_of_jobs:\",number_of_jobs)\n",
    "print(\"number_of_pages:\",number_of_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_job_ids(soup):\n",
    "    ids_on_page = []\n",
    "\n",
    "    job_postings = soup.find_all('li', {'class': 'ember-view'})\n",
    "\n",
    "    for job_posting in job_postings:\n",
    "        job_id = job_posting.get('data-occludable-job-id')\n",
    "        ids_on_page.append(job_id)\n",
    "\n",
    "    return ids_on_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find job ids on the first page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4137245989', '3936580453', '4063857099', '4122081493', '4121139946', '4133033481', '4139199423', '3935983004', '4120976805', '4132048865', '4121134848', '4139845618', '4082686855', '4129433849', '4131020637', '4072902328', '4144819234', '4144820138', '4144434105', '4142359405', '4144343176', '4145588902', '4144441725', '4143224517', '4143223837']\n",
      "Jobs on page:  25\n"
     ]
    }
   ],
   "source": [
    "first_page_jobs = find_job_ids(soup)\n",
    "filtered_job_ids = [job_id for job_id in first_page_jobs if job_id is not None]  # filter them just in case if there's something wrong\n",
    "print(filtered_job_ids)\n",
    "print(\"Jobs on page: \", len(filtered_job_ids))\n",
    "\n",
    "job_ids.extend(filtered_job_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case if number of pages is more than 1, iterate over the remaining pages and get the job ids\n",
    "\n",
    "Save extracted job ids in csv file\n",
    "\n",
    "Can change the value of the sleep_time based on internet speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page: 2...['4144900760', '4144431991', '3787789000', '4042554544', '4144909903', '4120978886', '4142361143', '4072487387', '4142319103', '4142312846', '4130151238', '4142330252']\n",
      "Jobs on page:12\n",
      "Saved job ids in csv file\n"
     ]
    }
   ],
   "source": [
    "if number_of_pages>1:\n",
    "    for page_num in range(1,number_of_pages):\n",
    "        print(f\"Scraping page: {page_num + 1}\",end=\"...\")\n",
    "        start = 25 * page_num\n",
    "        \n",
    "        url = f'https://www.linkedin.com/jobs/search/?keywords={keywords}&location={location}&start={start}'\n",
    "        url = requests.utils.requote_uri(url)\n",
    "        driver.get(url)\n",
    "        scroll_to_bottom(driver, sleep_time=120)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        current_page_jobs = find_job_ids(soup)\n",
    "        filtered_job_ids = [job_id for job_id in current_page_jobs if job_id is not None]\n",
    "        print(filtered_job_ids)\n",
    "        job_ids.extend(filtered_job_ids)  \n",
    "        print(f'Jobs on page:{len(filtered_job_ids)}')\n",
    "\n",
    "pd.DataFrame({\"Job_Id\":job_ids}).to_csv('../data/job_ids.csv',index=False)\n",
    "print('Saved job ids in csv file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Scrapping job description using Requests and BeautifulSoup\n",
    "\n",
    "https://www.scrapingdog.com/blog/scrape-linkedin-jobs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "list_job_ids = pd.read_csv(\"../data/job_ids.csv\").Job_Id.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4137245989, 3936580453, 4063857099, 4122081493, 4121139946]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(list_job_ids))\n",
    "list_job_ids[:5] # got only the first 5 to check if everything is ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function designed to remove HTML tags while keeping the visible text context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    " \n",
    "    for data in soup(['style', 'script']):\n",
    "        data.decompose()\n",
    " \n",
    "    return ' '.join(soup.stripped_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the code takes each job ID from list_job_ids, constructs the job's LinkedIn API URL, makes a request to fetch the job listing details, and extracts relevant information from the HTML response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ... read jobId:4137245989\n",
      "2 ... read jobId:3936580453\n",
      "3 ... read jobId:4063857099\n",
      "4 ... read jobId:4122081493\n",
      "5 ... read jobId:4121139946\n",
      "6 ... read jobId:4133033481\n",
      "7 ... read jobId:4139199423\n",
      "8 ... read jobId:3935983004\n",
      "9 ... read jobId:4120976805\n",
      "10 ... read jobId:4132048865\n",
      "11 ... read jobId:4121134848\n",
      "12 ... read jobId:4139845618\n",
      "13 ... read jobId:4082686855\n",
      "14 ... read jobId:4129433849\n",
      "15 ... read jobId:4131020637\n",
      "16 ... read jobId:4072902328\n",
      "17 ... read jobId:4144819234\n",
      "18 ... read jobId:4144820138\n",
      "19 ... read jobId:4144434105\n",
      "20 ... read jobId:4142359405\n",
      "21 ... read jobId:4144343176\n",
      "22 ... read jobId:4145588902\n",
      "23 ... read jobId:4144441725\n",
      "24 ... read jobId:4143224517\n",
      "25 ... read jobId:4143223837\n",
      "26 ... read jobId:4144900760\n",
      "27 ... read jobId:4144431991\n",
      "28 ... read jobId:3787789000\n",
      "29 ... read jobId:4042554544\n",
      "30 ... read jobId:4144909903\n",
      "31 ... read jobId:4120978886\n",
      "32 ... read jobId:4142361143\n",
      "33 ... read jobId:4072487387\n",
      "34 ... read jobId:4142319103\n",
      "35 ... read jobId:4142312846\n",
      "36 ... read jobId:4130151238\n",
      "37 ... read jobId:4142330252\n"
     ]
    }
   ],
   "source": [
    "job_url = 'https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{}'\n",
    "list_jobs = []\n",
    "\n",
    "# LinkedIn might block frequent requests, so it's safer to add User-Agent header to avoid detection as a bot\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def safe_find_text(soup, tag, attrs):\n",
    "    element = soup.find(tag, attrs)\n",
    "    return element.text.strip() if element else None\n",
    "\n",
    "for j in range(len(list_job_ids)):\n",
    "    print(f\"{j+1} ... read jobId:{list_job_ids[j]}\")\n",
    "\n",
    "    time.sleep(1)\n",
    "    resp = requests.get(job_url.format(list_job_ids[j]), headers=headers)\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    \n",
    "    job = {}\n",
    "\n",
    "    job[\"Job_ID\"] = list_job_ids[j]\n",
    "    job[\"Job_txt\"] = remove_tags(resp.text)\n",
    "\n",
    "    try:\n",
    "        job[\"company\"]=soup.find(\"div\",{\"class\":\"top-card-layout__card\"}).find(\"a\").find(\"img\").get('alt')\n",
    "    except:\n",
    "        job[\"company\"]=None\n",
    "\n",
    "    try:\n",
    "        job[\"job-title\"]=soup.find(\"div\",{\"class\":\"top-card-layout__entity-info\"}).find(\"a\").text.strip()\n",
    "    except:\n",
    "        job[\"job-title\"]=None\n",
    "\n",
    "    try:\n",
    "        job[\"level\"]=soup.find(\"ul\",{\"class\":\"description__job-criteria-list\"}).find(\"li\").text.replace(\"Seniority level\",\"\").strip()\n",
    "    except:\n",
    "        job[\"level\"]=None\n",
    "\n",
    "    try:\n",
    "        job[\"location\"]=soup.find(\"span\",{\"class\":\"topcard__flavor topcard__flavor--bullet\"}).text.strip()\n",
    "    except:\n",
    "        job[\"location\"]=None\n",
    "    \n",
    "    try:\n",
    "        job[\"posted-time-ago\"]=soup.find(\"span\",{\"class\":\"posted-time-ago__text topcard__flavor--metadata\"}).text.strip()\n",
    "    except:\n",
    "        job[\"posted-time-ago\"]=None\n",
    "\n",
    "    try:\n",
    "        nb_candidats_text = soup.find(\"span\", {\"class\": \"num-applicants__caption\"}).text.strip()\n",
    "        job[\"nb_candidats\"] = int(nb_candidats_text.split()[0])\n",
    "    except:\n",
    "        job[\"nb_candidats\"] = None\n",
    "\n",
    "    list_jobs.append(job)\n",
    "\n",
    "jobs_DF = pd.DataFrame(list_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_ID</th>\n",
       "      <th>Job_txt</th>\n",
       "      <th>company</th>\n",
       "      <th>job-title</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>posted-time-ago</th>\n",
       "      <th>nb_candidats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4137245989</td>\n",
       "      <td>Junior Software Developer IBM Sofia, Sofia Cit...</td>\n",
       "      <td>IBM</td>\n",
       "      <td>Junior Software Developer</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Sofia, Sofia City, Bulgaria</td>\n",
       "      <td>1 week ago</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3936580453</td>\n",
       "      <td>Junior Software Engineer Trading 212 Sofia, So...</td>\n",
       "      <td>Trading 212</td>\n",
       "      <td>Junior Software Engineer</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Sofia, Sofia City, Bulgaria</td>\n",
       "      <td>1 week ago</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4063857099</td>\n",
       "      <td>Junior Python Developer DXC Technology Sofia, ...</td>\n",
       "      <td>DXC Technology</td>\n",
       "      <td>Junior Python Developer</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Sofia, Sofia City, Bulgaria</td>\n",
       "      <td>3 months ago</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4122081493</td>\n",
       "      <td>Junior Java Engineer Dreamix Sofia, Sofia City...</td>\n",
       "      <td>Dreamix</td>\n",
       "      <td>Junior Java Engineer</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Sofia, Sofia City, Bulgaria</td>\n",
       "      <td>3 weeks ago</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4121139946</td>\n",
       "      <td>Junior Backend Developer LimeChain - Blockchai...</td>\n",
       "      <td>LimeChain - Blockchain &amp; Web3 Solutions</td>\n",
       "      <td>Junior Backend Developer</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Sofia Metropolitan Area</td>\n",
       "      <td>3 weeks ago</td>\n",
       "      <td>174.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Job_ID                                            Job_txt  \\\n",
       "0  4137245989  Junior Software Developer IBM Sofia, Sofia Cit...   \n",
       "1  3936580453  Junior Software Engineer Trading 212 Sofia, So...   \n",
       "2  4063857099  Junior Python Developer DXC Technology Sofia, ...   \n",
       "3  4122081493  Junior Java Engineer Dreamix Sofia, Sofia City...   \n",
       "4  4121139946  Junior Backend Developer LimeChain - Blockchai...   \n",
       "\n",
       "                                   company                  job-title  \\\n",
       "0                                      IBM  Junior Software Developer   \n",
       "1                              Trading 212   Junior Software Engineer   \n",
       "2                           DXC Technology    Junior Python Developer   \n",
       "3                                  Dreamix       Junior Java Engineer   \n",
       "4  LimeChain - Blockchain & Web3 Solutions   Junior Backend Developer   \n",
       "\n",
       "         level                     location posted-time-ago  nb_candidats  \n",
       "0  Entry level  Sofia, Sofia City, Bulgaria      1 week ago          41.0  \n",
       "1  Entry level  Sofia, Sofia City, Bulgaria      1 week ago           NaN  \n",
       "2  Entry level  Sofia, Sofia City, Bulgaria    3 months ago           NaN  \n",
       "3  Entry level  Sofia, Sofia City, Bulgaria     3 weeks ago           NaN  \n",
       "4  Entry level      Sofia Metropolitan Area     3 weeks ago         174.0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All LinkedIn job details are scraped, so the next step is to process the data, create new columns(like posted_time_ago) and clean job_description as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below cleans the job description by removing unwanted text that comes from LinkedIn's interface elements, such as login prompts, buttons, and legal disclaimers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_job_description(text):\n",
    "    senetences_to_remove = [\"Remove photo First name Last name Email Password (8+ characters) \",\n",
    "                            \"By clicking Agree & Join\",\n",
    "                            \"you agree to the LinkedIn User Agreement\",\n",
    "                            \"Privacy Policy and Cookie Policy\",\n",
    "                            \"Continue Agree & Join or Apply on company website\",\n",
    "                            \"Security verification\",\n",
    "                            \"Close Already on LinkedIn ?\",\n",
    "                            \"Close Already on LinkedIn?\",\n",
    "                            \"Sign in Save Save job Save this job with your existing LinkedIn profile , or create a new one\",\n",
    "                            \"Sign in Save Save job Save this job with your existing LinkedIn profile, or create a new one\",\n",
    "                            \"Your job seeking activity is only visible to you\",\n",
    "                            \"Email Continue Welcome back\"]\n",
    "    for sentence in senetences_to_remove:\n",
    "        result = text.find(sentence)\n",
    "        if result > -1:\n",
    "            text = text[:result] + text[result+len(sentence):]\n",
    "\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posted_date(posted_time_ago,date_scraping):\n",
    "    \"\"\"Convert posted_time_ago to number of days.\n",
    "    For example, 1 month ago is replaced by 30. 1 week by 7 and so on...\"\"\"\n",
    "    posted_date = None\n",
    "    \n",
    "    try:\n",
    "        details = posted_time_ago.split()\n",
    "        N_DAYS_AGO = int(details[0])\n",
    "        day_week_month_year = details[1] \n",
    "        if day_week_month_year.startswith(\"day\"):\n",
    "            N_DAYS_AGO = N_DAYS_AGO\n",
    "        elif day_week_month_year.startswith(\"week\"):\n",
    "            N_DAYS_AGO = N_DAYS_AGO*7\n",
    "        elif day_week_month_year.startswith(\"month\"):\n",
    "            N_DAYS_AGO = N_DAYS_AGO*30\n",
    "        elif day_week_month_year.startswith(\"year\"):\n",
    "            N_DAYS_AGO = N_DAYS_AGO*365\n",
    "        else:\n",
    "            N_DAYS_AGO = None\n",
    "\n",
    "        posted_date = date_scraping - datetime.timedelta(days=N_DAYS_AGO)\n",
    "    except:\n",
    "        posted_date = None\n",
    "\n",
    "    return posted_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_ID</th>\n",
       "      <th>Job_txt</th>\n",
       "      <th>company</th>\n",
       "      <th>job-title</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>posted-time-ago</th>\n",
       "      <th>nb_candidats</th>\n",
       "      <th>scraping_date</th>\n",
       "      <th>posted_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4137245989</td>\n",
       "      <td>Junior Software Developer IBM Sofia, Sofia Cit...</td>\n",
       "      <td>IBM</td>\n",
       "      <td>Junior Software Developer</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Sofia, Sofia City, Bulgaria</td>\n",
       "      <td>1 week ago</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2025-02-06</td>\n",
       "      <td>2025-01-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3936580453</td>\n",
       "      <td>Junior Software Engineer Trading 212 Sofia, So...</td>\n",
       "      <td>Trading 212</td>\n",
       "      <td>Junior Software Engineer</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Sofia, Sofia City, Bulgaria</td>\n",
       "      <td>1 week ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-02-06</td>\n",
       "      <td>2025-01-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4063857099</td>\n",
       "      <td>Junior Python Developer DXC Technology Sofia, ...</td>\n",
       "      <td>DXC Technology</td>\n",
       "      <td>Junior Python Developer</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Sofia, Sofia City, Bulgaria</td>\n",
       "      <td>3 months ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-02-06</td>\n",
       "      <td>2024-11-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4122081493</td>\n",
       "      <td>Junior Java Engineer Dreamix Sofia, Sofia City...</td>\n",
       "      <td>Dreamix</td>\n",
       "      <td>Junior Java Engineer</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Sofia, Sofia City, Bulgaria</td>\n",
       "      <td>3 weeks ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-02-06</td>\n",
       "      <td>2025-01-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4121139946</td>\n",
       "      <td>Junior Backend Developer LimeChain - Blockchai...</td>\n",
       "      <td>LimeChain - Blockchain &amp; Web3 Solutions</td>\n",
       "      <td>Junior Backend Developer</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Sofia Metropolitan Area</td>\n",
       "      <td>3 weeks ago</td>\n",
       "      <td>174.0</td>\n",
       "      <td>2025-02-06</td>\n",
       "      <td>2025-01-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Job_ID                                            Job_txt  \\\n",
       "0  4137245989  Junior Software Developer IBM Sofia, Sofia Cit...   \n",
       "1  3936580453  Junior Software Engineer Trading 212 Sofia, So...   \n",
       "2  4063857099  Junior Python Developer DXC Technology Sofia, ...   \n",
       "3  4122081493  Junior Java Engineer Dreamix Sofia, Sofia City...   \n",
       "4  4121139946  Junior Backend Developer LimeChain - Blockchai...   \n",
       "\n",
       "                                   company                  job-title  \\\n",
       "0                                      IBM  Junior Software Developer   \n",
       "1                              Trading 212   Junior Software Engineer   \n",
       "2                           DXC Technology    Junior Python Developer   \n",
       "3                                  Dreamix       Junior Java Engineer   \n",
       "4  LimeChain - Blockchain & Web3 Solutions   Junior Backend Developer   \n",
       "\n",
       "         level                     location posted-time-ago  nb_candidats  \\\n",
       "0  Entry level  Sofia, Sofia City, Bulgaria      1 week ago          41.0   \n",
       "1  Entry level  Sofia, Sofia City, Bulgaria      1 week ago           NaN   \n",
       "2  Entry level  Sofia, Sofia City, Bulgaria    3 months ago           NaN   \n",
       "3  Entry level  Sofia, Sofia City, Bulgaria     3 weeks ago           NaN   \n",
       "4  Entry level      Sofia Metropolitan Area     3 weeks ago         174.0   \n",
       "\n",
       "  scraping_date posted_date  \n",
       "0    2025-02-06  2025-01-30  \n",
       "1    2025-02-06  2025-01-30  \n",
       "2    2025-02-06  2024-11-08  \n",
       "3    2025-02-06  2025-01-16  \n",
       "4    2025-02-06  2025-01-16  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_DF['scraping_date'] = pd.to_datetime(datetime.date.today())\n",
    "jobs_DF['posted_date'] = np.vectorize(get_posted_date)(jobs_DF['posted-time-ago'], jobs_DF['scraping_date'])\n",
    "\n",
    "jobs_DF['Job_txt'] = jobs_DF['Job_txt'].apply(clean_job_description)\n",
    "jobs_DF.level = jobs_DF.level.apply(lambda x:x.replace(\"Employment type\\n        \\n\\n          \",\"\") if x is not None else x)\n",
    "\n",
    "jobs_DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_DF.to_json(\"../data/linkedin_jobs_scraped.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
